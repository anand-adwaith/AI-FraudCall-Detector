{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12266812,"sourceType":"datasetVersion","datasetId":7730003},{"sourceId":447658,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":363431,"modelId":384299}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import requests\nresponse = requests.get(\"https://huggingface.co\")\nprint(response.status_code)  # Should print 200","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T11:08:41.187784Z","iopub.execute_input":"2025-06-24T11:08:41.188034Z","iopub.status.idle":"2025-06-24T11:08:41.403205Z","shell.execute_reply.started":"2025-06-24T11:08:41.188015Z","shell.execute_reply":"2025-06-24T11:08:41.402488Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nhf_token = user_secrets.get_secret(\"HF_TOKEN\")\nfrom huggingface_hub import login\nlogin(token=hf_token)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T11:09:30.852682Z","iopub.execute_input":"2025-06-24T11:09:30.853204Z","iopub.status.idle":"2025-06-24T11:09:31.453989Z","shell.execute_reply.started":"2025-06-24T11:09:30.853181Z","shell.execute_reply":"2025-06-24T11:09:31.453480Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -q transformers peft accelerate bitsandbytes datasets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T11:09:35.234444Z","iopub.execute_input":"2025-06-24T11:09:35.234905Z","iopub.status.idle":"2025-06-24T11:10:52.962470Z","shell.execute_reply.started":"2025-06-24T11:09:35.234884Z","shell.execute_reply":"2025-06-24T11:10:52.961756Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!nvidia-smi","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T11:10:54.997667Z","iopub.execute_input":"2025-06-24T11:10:54.998229Z","iopub.status.idle":"2025-06-24T11:10:55.223422Z","shell.execute_reply.started":"2025-06-24T11:10:54.998199Z","shell.execute_reply":"2025-06-24T11:10:55.222716Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nfrom datasets import Dataset\n\n# Load data\ndf = pd.read_excel(\"/kaggle/input/aiscam-dataset/SyntheticDataSet_Call_Text_5.xlsx\", engine=\"openpyxl\")\n\n# Clean data\ndf = df[['text', 'label']].dropna()\ndf['label'] = df['label'].str.strip().str.lower().map({'fraud': 'fraud', 'normal': 'normal'})\n\n# Prompt-format the dataset\ndf['prompt'] = df['text'].apply(lambda x: f\"Classify this call:\\n{x}\\nLabel:\")\ndf['response'] = df['label']\n\n# Convert to Hugging Face Dataset\ndataset = Dataset.from_pandas(df[['prompt', 'response']], preserve_index=False)\n\n# Train-test split\ndataset = dataset.train_test_split(test_size=0.1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T11:11:38.056560Z","iopub.execute_input":"2025-06-24T11:11:38.057463Z","iopub.status.idle":"2025-06-24T11:11:40.127813Z","shell.execute_reply.started":"2025-06-24T11:11:38.057419Z","shell.execute_reply":"2025-06-24T11:11:40.127284Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import shutil\nimport os\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\n# Copy Gemma model to writable path\nsrc_path = \"/kaggle/input/gemma/transformers/default/1/gemma-2b\"\ndst_path = \"/kaggle/working/gemma-2b\"\nif not os.path.exists(dst_path):\n    shutil.copytree(src_path, dst_path)\n\n# Load model + tokenizer\ntokenizer = AutoTokenizer.from_pretrained(dst_path, use_fast=True, local_files_only=True)\nmodel = AutoModelForCausalLM.from_pretrained(dst_path, torch_dtype=torch.float16, device_map=\"auto\", local_files_only=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T11:12:06.623795Z","iopub.execute_input":"2025-06-24T11:12:06.624504Z","iopub.status.idle":"2025-06-24T11:14:50.090830Z","shell.execute_reply.started":"2025-06-24T11:12:06.624480Z","shell.execute_reply":"2025-06-24T11:14:50.090269Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def tokenize(example):\n    return tokenizer(\n        example['prompt'],\n        text_target=example['response'],\n        truncation=True,\n        padding=\"max_length\",\n        max_length=256\n    )\n\ntokenized = dataset.map(tokenize, batched=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T11:14:57.363371Z","iopub.execute_input":"2025-06-24T11:14:57.363652Z","iopub.status.idle":"2025-06-24T11:15:00.426389Z","shell.execute_reply.started":"2025-06-24T11:14:57.363631Z","shell.execute_reply":"2025-06-24T11:15:00.425570Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from peft import get_peft_model, LoraConfig, TaskType\n\nlora_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    lora_dropout=0.1,\n    bias=\"none\",\n    task_type=TaskType.CAUSAL_LM,\n    target_modules=[\"q_proj\", \"v_proj\"]\n)\n\nmodel = get_peft_model(model, lora_config)\nmodel.print_trainable_parameters()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T11:15:03.441243Z","iopub.execute_input":"2025-06-24T11:15:03.441888Z","iopub.status.idle":"2025-06-24T11:15:04.676122Z","shell.execute_reply.started":"2025-06-24T11:15:03.441865Z","shell.execute_reply":"2025-06-24T11:15:04.675416Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import time\nfrom transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n\ntraining_args = TrainingArguments(\n    output_dir=\"/kaggle/working/gemma2b-finetuned\",\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=2,\n    per_device_eval_batch_size=2,\n    num_train_epochs=2,\n    logging_steps=10,\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    learning_rate=2e-4,\n    save_total_limit=1,\n    fp16=True,\n    report_to=\"none\"\n)\n\ndata_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized[\"train\"],\n    eval_dataset=tokenized[\"test\"],\n    tokenizer=tokenizer,\n    data_collator=data_collator\n)\n\nstart = time.time()\nprint(\"Training started...\")\ntrainer.train()\nprint(f\"âœ… Training completed in {(time.time()-start)/60:.2f} minutes\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T11:26:11.063036Z","iopub.execute_input":"2025-06-24T11:26:11.063762Z","iopub.status.idle":"2025-06-24T12:10:55.493633Z","shell.execute_reply.started":"2025-06-24T11:26:11.063733Z","shell.execute_reply":"2025-06-24T12:10:55.492496Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"!ps -ef ","metadata":{"execution":{"iopub.status.busy":"2025-06-24T11:16:18.359271Z","iopub.execute_input":"2025-06-24T11:16:18.359796Z","iopub.status.idle":"2025-06-24T11:16:18.558064Z","shell.execute_reply.started":"2025-06-24T11:16:18.359774Z","shell.execute_reply":"2025-06-24T11:16:18.557260Z"}}},{"cell_type":"markdown","source":"!nvidia-smi\n","metadata":{"execution":{"iopub.status.busy":"2025-06-24T11:24:48.144599Z","iopub.execute_input":"2025-06-24T11:24:48.145437Z","iopub.status.idle":"2025-06-24T11:24:48.426335Z","shell.execute_reply.started":"2025-06-24T11:24:48.145410Z","shell.execute_reply":"2025-06-24T11:24:48.425532Z"}}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\n\n# Load fine-tuned model from saved directory\nmodel_dir = \"/kaggle/working/gemma2b-finetuned\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_dir, local_files_only=True)\nmodel = AutoModelForCausalLM.from_pretrained(model_dir, local_files_only=True)\nmodel.eval()\n\n# Auto-select device (GPU if available)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T12:49:00.984146Z","iopub.execute_input":"2025-06-24T12:49:00.984413Z","iopub.status.idle":"2025-06-24T12:49:01.039470Z","shell.execute_reply.started":"2025-06-24T12:49:00.984396Z","shell.execute_reply":"2025-06-24T12:49:01.038590Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def classify(text):\n    prompt = f\"Classify this call:\\n{text}\\nLabel:\"\n    \n    # Tokenize input\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n\n    # Generate model output\n    output = model.generate(\n        **inputs,\n        max_new_tokens=5,\n        do_sample=False,\n        pad_token_id=tokenizer.eos_token_id\n    )\n\n    # Decode and extract label\n    decoded = tokenizer.decode(output[0], skip_special_tokens=True)\n    label = decoded.split(\"Label:\")[-1].strip().split()[0].lower()\n\n    if \"fraud\" in label:\n        return \"fraud\"\n    elif \"normal\" in label or \"legit\" in label:\n        return \"normal\"\n    else:\n        return f\"Uncertain: {label}\"\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"!zip -r gemma2b-finetuned.zip /kaggle/working/gemma2b-finetuned","metadata":{"execution":{"iopub.status.busy":"2025-06-24T12:35:29.375782Z","iopub.execute_input":"2025-06-24T12:35:29.376402Z","iopub.status.idle":"2025-06-24T12:35:31.575663Z","shell.execute_reply.started":"2025-06-24T12:35:29.376378Z","shell.execute_reply":"2025-06-24T12:35:31.574683Z"}}},{"cell_type":"markdown","source":"from IPython.display import FileLink\nFileLink(r'gemma2b-finetuned.zip')","metadata":{"execution":{"iopub.status.busy":"2025-06-24T12:35:49.078305Z","iopub.execute_input":"2025-06-24T12:35:49.078908Z","iopub.status.idle":"2025-06-24T12:35:49.084349Z","shell.execute_reply.started":"2025-06-24T12:35:49.078877Z","shell.execute_reply":"2025-06-24T12:35:49.083659Z"}}}]}
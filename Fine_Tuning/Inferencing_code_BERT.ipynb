{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12e622f0-5672-43dc-9f50-df137d5a116b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9100\n",
      "Precision: 1.0000\n",
      "Recall: 0.8393\n",
      "F1-Score: 0.9126\n",
      "Response Time: 3.97 seconds\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      normal       0.83      1.00      0.91        44\n",
      "       fraud       1.00      0.84      0.91        56\n",
      "\n",
      "    accuracy                           0.91       100\n",
      "   macro avg       0.92      0.92      0.91       100\n",
      "weighted avg       0.93      0.91      0.91       100\n",
      "\n",
      "\n",
      "Sample Predictions:\n",
      "--------------------------------------------------\n",
      "Sample Index    Correct Label   Predicted Label\n",
      "--------------------------------------------------\n",
      "0               fraud           normal         \n",
      "1               fraud           normal         \n",
      "2               fraud           normal         \n",
      "3               fraud           normal         \n",
      "4               fraud           fraud          \n",
      "5               fraud           fraud          \n",
      "6               fraud           normal         \n",
      "7               fraud           fraud          \n",
      "8               fraud           fraud          \n",
      "9               fraud           fraud          \n",
      "10              fraud           fraud          \n",
      "11              fraud           fraud          \n",
      "12              fraud           fraud          \n",
      "13              fraud           fraud          \n",
      "14              fraud           fraud          \n",
      "15              fraud           fraud          \n",
      "16              fraud           fraud          \n",
      "17              fraud           fraud          \n",
      "18              fraud           fraud          \n",
      "19              fraud           fraud          \n",
      "20              fraud           fraud          \n",
      "21              fraud           normal         \n",
      "22              fraud           fraud          \n",
      "23              fraud           fraud          \n",
      "24              fraud           normal         \n",
      "25              fraud           fraud          \n",
      "26              fraud           normal         \n",
      "27              fraud           fraud          \n",
      "28              fraud           normal         \n",
      "29              fraud           fraud          \n",
      "30              fraud           fraud          \n",
      "31              fraud           fraud          \n",
      "32              fraud           fraud          \n",
      "33              fraud           fraud          \n",
      "34              fraud           fraud          \n",
      "35              fraud           fraud          \n",
      "36              fraud           fraud          \n",
      "37              fraud           fraud          \n",
      "38              fraud           fraud          \n",
      "39              fraud           fraud          \n",
      "40              fraud           fraud          \n",
      "41              fraud           fraud          \n",
      "42              fraud           fraud          \n",
      "43              fraud           fraud          \n",
      "44              fraud           fraud          \n",
      "45              fraud           fraud          \n",
      "46              fraud           fraud          \n",
      "47              fraud           fraud          \n",
      "48              fraud           fraud          \n",
      "49              fraud           fraud          \n",
      "50              fraud           fraud          \n",
      "51              fraud           fraud          \n",
      "52              fraud           fraud          \n",
      "53              fraud           fraud          \n",
      "54              fraud           fraud          \n",
      "55              fraud           fraud          \n",
      "56              normal          normal         \n",
      "57              normal          normal         \n",
      "58              normal          normal         \n",
      "59              normal          normal         \n",
      "60              normal          normal         \n",
      "61              normal          normal         \n",
      "62              normal          normal         \n",
      "63              normal          normal         \n",
      "64              normal          normal         \n",
      "65              normal          normal         \n",
      "66              normal          normal         \n",
      "67              normal          normal         \n",
      "68              normal          normal         \n",
      "69              normal          normal         \n",
      "70              normal          normal         \n",
      "71              normal          normal         \n",
      "72              normal          normal         \n",
      "73              normal          normal         \n",
      "74              normal          normal         \n",
      "75              normal          normal         \n",
      "76              normal          normal         \n",
      "77              normal          normal         \n",
      "78              normal          normal         \n",
      "79              normal          normal         \n",
      "80              normal          normal         \n",
      "81              normal          normal         \n",
      "82              normal          normal         \n",
      "83              normal          normal         \n",
      "84              normal          normal         \n",
      "85              normal          normal         \n",
      "86              normal          normal         \n",
      "87              normal          normal         \n",
      "88              normal          normal         \n",
      "89              normal          normal         \n",
      "90              normal          normal         \n",
      "91              normal          normal         \n",
      "92              normal          normal         \n",
      "93              normal          normal         \n",
      "94              normal          normal         \n",
      "95              normal          normal         \n",
      "96              normal          normal         \n",
      "97              normal          normal         \n",
      "98              normal          normal         \n",
      "99              normal          normal         \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Define custom dataset class\n",
    "class TextClassificationDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.label_map = {'normal': 0, 'fraud': 1}  # Adjust if your labels differ\n",
    "        self.reverse_label_map = {0: 'normal', 1: 'fraud'}  # For decoding predictions\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.label_map[self.labels[idx]]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long),\n",
    "            'index': idx  # Store index to track samples\n",
    "        }\n",
    "\n",
    "# Load test dataset\n",
    "def load_test_data(file_path):\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, encoding='cp1252')  # Use Windows-1252 encoding\n",
    "    except UnicodeDecodeError:\n",
    "        print(\"Error: Unable to decode file with cp1252 encoding. Trying latin1 encoding...\")\n",
    "        df = pd.read_csv(file_path, encoding='latin1')  # Fallback to latin1\n",
    "    texts = df['text'].values\n",
    "    labels = df['label'].values\n",
    "    return texts, labels\n",
    "\n",
    "# Evaluate model\n",
    "def evaluate_model(model, dataloader, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    sample_indices = []\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            indices = batch['index']\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "\n",
    "            predictions.extend(preds)\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "            sample_indices.extend(indices.numpy())\n",
    "\n",
    "    end_time = time.time()\n",
    "    response_time = end_time - start_time\n",
    "\n",
    "    # Compute metrics\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(true_labels, predictions, average='binary')\n",
    "    report = classification_report(true_labels, predictions, target_names=['normal', 'fraud'])\n",
    "\n",
    "    return accuracy, precision, recall, f1, report, response_time, sample_indices, true_labels, predictions\n",
    "\n",
    "def main():\n",
    "    # Configuration\n",
    "    model_path = r\"C:\\Users\\HAN4COB\\.conda\\envs\\test\\AI_Enabled_Scam_Call_Detection\\bert-spam-model2\"  # Update with your model directory path\n",
    "    test_file = 'Test_Dataset.csv'\n",
    "    max_len = 128\n",
    "    batch_size = 16\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Load tokenizer and model\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_path, use_safetensors=True)\n",
    "    model = BertForSequenceClassification.from_pretrained(model_path, use_safetensors=True)\n",
    "    model.to(device)\n",
    "\n",
    "    # Load test data\n",
    "    texts, labels = load_test_data(test_file)\n",
    "\n",
    "    # Create test dataset and dataloader\n",
    "    test_dataset = TextClassificationDataset(texts, labels, tokenizer, max_len)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Evaluate\n",
    "    accuracy, precision, recall, f1, report, response_time, sample_indices, true_labels, predictions = evaluate_model(model, test_dataloader, device)\n",
    "\n",
    "    # Convert numeric labels back to strings\n",
    "    reverse_label_map = {0: 'normal', 1: 'fraud'}\n",
    "    true_labels_str = [reverse_label_map[label] for label in true_labels]\n",
    "    predictions_str = [reverse_label_map[pred] for pred in predictions]\n",
    "\n",
    "    # Print results\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1-Score: {f1:.4f}\")\n",
    "    print(f\"Response Time: {response_time:.2f} seconds\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(report)\n",
    "\n",
    "    # Print correct and predicted labels for each sample\n",
    "    print(\"\\nSample Predictions:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"{'Sample Index':<15} {'Correct Label':<15} {'Predicted Label':<15}\")\n",
    "    print(\"-\" * 50)\n",
    "    for idx, true_label, pred_label in zip(sample_indices, true_labels_str, predictions_str):\n",
    "        print(f\"{idx:<15} {true_label:<15} {pred_label:<15}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40eb882b-f2c2-40bd-a80e-fb9dbec6b41c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Test",
   "language": "python",
   "name": "test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
